# This config file runs the MLP-based RL chess agent model with debug training parameters
model: "MLP_Agent_Debug" # Name of the model being trained, determines the output directory name
model_class: "MLP" # Should match one of the model classes in core/torch_models.py


output:
    output_path: &output_path "results/MLP_Agent_Debug/" # Directory of training results
    model_output: !join_path [*output_path, "saved_weights/"] # Location of saved model weights
    log_path: !join_path [*output_path, "log.txt"] # Location of training log
    plot_output: *output_path # Location of eval scores during training summary plot and csv data
    record_path: !join_path [*output_path, "recordings/"] # Location of where to save episode recordings
    tensorboard: !join_path [*output_path, "tensorboard/"] # Location to save tensorboard log files
    clear_all: True # If set to True, then the output_path is cleared at the start, deletes existing results

search_func: # This defines kwargs that will be passed to the search function from core/search_algos.py
    name: "naive_search"
    batch_size: 32 # The batch_size used for generating state value estimates

model_training:
    record: True
    seed: 2025 # Set a random seed for usage in the replay buffer
    load_dir: !join_path [*output_path, "saved_weights/"] # Location to look for pre-trained saved weights
    num_episodes_test: 1 # The number of episodes to run for each periodic evaluation step
    clip_val: 1.0 # The norm of the gradient vector used in grad clipping
    saving_freq: 25 # The frequency of saving model weights measured in training timesteps
    log_freq: 10 # The frequency of log updates during training measured in training timesteps
    eval_freq: 100 # The frequency of evaluation runs measured in training timesteps
    max_eval_moves: 10 # Set the max number of moves per eval self-play game
    compile: False # Whether to compile the model before training
    compile_mode: "default" # The compile mode to use if compile is True

hyper_params:
    nsteps_train: 5000 # The number of total training timesteps to run
    batch_size: 32 # The batch size when sampling training examples from the replay buffer
    buffer_size: 1000 # The frame capacity of the replay buffer
    gamma: 1.00 # The temporal discount factor of future rewards
    learning_freq: 100 # The frequency of update to the v_network model parameters
    learning_updates: 3 # The number of learning update batches run each at each update interval 
    learning_start: 100 # A warm-up period before any training begins
    wt_decay: 0.005 # Set an L2 regularization amount

    lr_begin: 0.00025 # The first value of the learning rate
    lr_end: 0.000025 # The final value of the learning rate
    lr_nsteps: 1500000 # The number of steps over which the learning rate is decayed
    
    eps_begin: 0.5 # The first value of the epsilon exploration probability
    eps_end: 0.01 # The final value of the epsilon exploration probability
    eps_nsteps: 2000000 # The number of steps over which the epsilon exploration probability is decayed
    
    eps: 1e-5 # The min priority value given to every transition for priorty sampling
    alpha: 0.6 # Controls the degree of prioritizing higher TD errors, alpha == 0 creates uniform sampling
    beta_begin: 0.4 # The first value of beta used in importance sampling
    beta_end: 1.0 # The final value of beta used in importance sampling
    beta_nsteps: 3000000 # The number of steps over which the beta for importance sampling is increased
